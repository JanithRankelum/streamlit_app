{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuY1uUoBAKrO4vLH061cbV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanithRankelum/streamlit_app/blob/master/app_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_iaAM-uHqmn"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# AWS S3 Configuration\n",
        "BUCKET_NAME = 'riceprice-s3-bucket'\n",
        "FILE_PATH = '/content/drive/MyDrive/cleaned_data.csv'\n",
        "\n",
        "# Initialize S3 client\n",
        "s3 = boto3.client('s3')\n",
        "\n",
        "# Function to upload a file to S3\n",
        "def upload_to_s3(file_path, bucket_name):\n",
        "    try:\n",
        "        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        s3_file_name = f'rice_prices_{timestamp}.csv'\n",
        "        s3.upload_file(file_path, bucket_name, s3_file_name)\n",
        "        st.success(f\"Uploaded {file_path} to S3 as {s3_file_name}\")\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error uploading file: {e}\")\n",
        "\n",
        "# Function to read the latest CSV file from S3\n",
        "def read_latest_csv_from_s3(bucket_name):\n",
        "    try:\n",
        "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
        "        if 'Contents' not in response:\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        latest_file = max(response['Contents'], key=lambda x: x['LastModified'])\n",
        "        file_key = latest_file['Key']\n",
        "        response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
        "        csv_data = response['Body'].read().decode('utf-8')\n",
        "        df = pd.read_csv(StringIO(csv_data))\n",
        "\n",
        "        # Ensure date is in datetime format\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = pd.to_datetime(df['date'])\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error reading file from S3: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Function to detect anomalies using Z-score\n",
        "def detect_anomalies(df, threshold=2.5):\n",
        "    if len(df) == 0:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = df.copy()\n",
        "    df['z_score'] = (df['price'] - df['price'].mean()) / df['price'].std()\n",
        "    df['is_anomaly'] = np.abs(df['z_score']) > threshold\n",
        "    return df\n",
        "\n",
        "# Function to calculate moving average\n",
        "def calculate_trends(df, window_size=3):\n",
        "    if len(df) == 0:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = df.sort_values('date')\n",
        "    df['moving_avg'] = df['price'].rolling(window=window_size).mean()\n",
        "    return df\n",
        "\n",
        "# Streamlit App\n",
        "st.title(\"Real-Time Rice Prices Dashboard\")\n",
        "st.write(\"Visualizing rice prices from S3 in real-time\")\n",
        "\n",
        "# Upload section\n",
        "if st.button(\"Upload Data to S3\"):\n",
        "    upload_to_s3(FILE_PATH, BUCKET_NAME)\n",
        "\n",
        "# Initialize session state\n",
        "if 'row_index' not in st.session_state:\n",
        "    st.session_state.row_index = 0\n",
        "if 'full_data' not in st.session_state:\n",
        "    st.session_state.full_data = pd.DataFrame()\n",
        "\n",
        "# Main display\n",
        "last_refresh = st.empty()\n",
        "\n",
        "while True:\n",
        "    # Load data\n",
        "    new_data = read_latest_csv_from_s3(BUCKET_NAME)\n",
        "    if not new_data.empty:\n",
        "        st.session_state.full_data = pd.concat([st.session_state.full_data, new_data]).drop_duplicates()\n",
        "\n",
        "    if not st.session_state.full_data.empty:\n",
        "        # Process data\n",
        "        processed_data = st.session_state.full_data.copy()\n",
        "        processed_data = calculate_trends(processed_data)\n",
        "        processed_data = detect_anomalies(processed_data)\n",
        "\n",
        "        # Get current window of data\n",
        "        start_idx = st.session_state.row_index\n",
        "        end_idx = min(start_idx + 10, len(processed_data))\n",
        "        current_data = processed_data.iloc[start_idx:end_idx]\n",
        "\n",
        "        # Display raw data\n",
        "        st.write(\"### Current Data Window\")\n",
        "        st.dataframe(current_data[['date', 'province', 'price']])\n",
        "\n",
        "        # Price Trends Visualization (Original Style)\n",
        "        st.write(\"### Price Trends\")\n",
        "        st.line_chart(\n",
        "            processed_data.set_index('date')[['price', 'moving_avg']],\n",
        "            use_container_width=True\n",
        "        )\n",
        "\n",
        "        # Anomaly Visualization (Original Style)\n",
        "        anomalies = processed_data[processed_data['is_anomaly']]\n",
        "        if not anomalies.empty:\n",
        "            st.write(\"### Detected Anomalies\")\n",
        "            st.dataframe(anomalies[['date', 'province', 'price', 'z_score']])\n",
        "\n",
        "            # Highlight anomalies on chart\n",
        "            st.write(\"### Anomalies on Price Chart\")\n",
        "            chart_data = processed_data.set_index('date')[['price']]\n",
        "            st.line_chart(chart_data)\n",
        "            for idx, row in anomalies.iterrows():\n",
        "                st.write(f\"ðŸš¨ Anomaly on {row['date'].date()}: {row['price']} (Z-score: {row['z_score']:.2f})\")\n",
        "\n",
        "        # Provincial Comparison (Original Style)\n",
        "        st.write(\"### Provincial Price Comparison\")\n",
        "        st.bar_chart(\n",
        "            current_data.groupby('province')['price'].mean()\n",
        "        )\n",
        "\n",
        "        # Summary Statistics\n",
        "        st.write(\"### Summary Statistics\")\n",
        "        st.write(current_data['price'].describe())\n",
        "\n",
        "        # Update index\n",
        "        st.session_state.row_index = end_idx\n",
        "        if st.session_state.row_index >= len(processed_data):\n",
        "            st.session_state.row_index = 0\n",
        "    else:\n",
        "        st.warning(\"No data available in S3 bucket\")\n",
        "\n",
        "    # Update refresh time\n",
        "    last_refresh.write(f\"Last refreshed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Wait for next refresh\n",
        "    time.sleep(60)  # Refresh every 60 seconds"
      ]
    }
  ]
}